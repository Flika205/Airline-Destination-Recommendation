{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = ''\n",
    "feature_set_name = ''\n",
    "features_dir = ''\n",
    "split_date = '' # date on which to split dataset into train and test data (alternatively, use Spark's randomSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = spark.read.option(\"header\", \"true\").option(\"schemaInfer\", \"true\").csv('%s/*' % source_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example column map mapping source data column names to relay42-industry concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    '': 'user_id',\n",
    "    '': 'item_id',\n",
    "    '': 'item_booking_date',\n",
    "    '': 'item_departure_date',\n",
    "    '': 'item_travel_class',\n",
    "    '': 'item_pax',\n",
    "    '': 'user_member',\n",
    "    '': 'user_gender',\n",
    "    '': 'num_bookings'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "ddf = ddf.select([col(k).alias(v) for k, v in column_map.items()]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "ddf = ddf.withColumn('user_dbd', datediff(ddf.item_departure_date, ddf.item_booking_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['item_pax_int']\n",
    "categorical_features = ['item_travel_class', 'user_gender', 'item_round_trip', 'user_member', 'user_infant', 'user_age_bracket']\n",
    "indexed_cat_cols = [c + '_idx' for c in categorical_features]\n",
    "ohe_cat_cols = [c.replace('_idx', '_') for c in indexed_cat_cols]\n",
    "user_features = list(filter(lambda c: c.startswith('user_'), ohe_cat_cols + numerical_features))\n",
    "item_features = list(filter(lambda c: c.startswith('item_'), ohe_cat_cols + numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, SQLTransformer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "casts = SQLTransformer(statement=\"SELECT *, CAST(item_pax AS int) item_pax_int, CAST(user_dbd AS int) user_dbd_int FROM __THIS__\")\n",
    "user_indexer = StringIndexer(inputCol='user_id', outputCol='user_idx', handleInvalid='keep')\n",
    "item_indexer = StringIndexer(inputCol='item_id', outputCol='item_idx', handleInvalid='keep')\n",
    "feature_indexers = [StringIndexer(inputCol=col, outputCol=col +'_idx', handleInvalid='keep') for col in categorical_features]\n",
    "onehotencoders = OneHotEncoderEstimator(inputCols=indexed_cat_cols, outputCols=ohe_cat_cols, handleInvalid='keep', dropLast=False)\n",
    "item_feature_assembler = VectorAssembler(inputCols=item_features, outputCol='item_features')\n",
    "user_feature_assembler = VectorAssembler(inputCols=user_features, outputCol='user_features')\n",
    "\n",
    "features_pipeline = Pipeline(stages=[casts] + [user_indexer] + [item_indexer] + feature_indexers + [onehotencoders] + [item_feature_assembler] + [user_feature_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = features_pipeline.fit(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "array_udf = udf(lambda value: value.toArray().tolist(), ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ddf = feature_model.transform(ddf).select(col('user_idx').cast('int').alias('user'), col('item_idx').cast('int').alias('item'), 'item_booking_date', 'item_departure_date', array_udf('item_features').alias('single_item_features'), array_udf('user_features').alias('single_user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model.stages[1].write().overwrite().save('%s/user_index_%s' % (feature_dir, feature_set_name))\n",
    "feature_model.stages[2].write().overwrite().save('%s/item_index_%s' % (feature_dir, feature_set_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unique users: %s; unique items: %s' % (feature_ddf.select('user').distinct().count(), feature_ddf.select('item').distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures_ddf = feature_ddf.groupBy('item_departure_date').count().withColumnRenamed('count', 'departures')\n",
    "conversions_ddf = feature_ddf.groupBy('item_booking_date').count().withColumnRenamed('count', 'conversions')\n",
    "display(departures_ddf.join(conversions_ddf, departures_ddf.item_departure_date == conversions_ddf.item_booking_date).withColumnRenamed('item_booking_date', 'date').select('date', 'conversions', 'departures').orderBy(col('date').asc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddf = feature_ddf.filter(feature_ddf.item_booking_date < split_date)\n",
    "test_ddf = feature_ddf.filter(feature_ddf.item_booking_date >= split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train observations: %s; evaluation observations: %s' % (train_ddf.count(), test_ddf.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "num_item_features = feature_ddf.select(size('single_item_features').alias('nif')).first().nif\n",
    "num_user_features = feature_ddf.select(size('single_user_features').alias('nuf')).first().nuf\n",
    "print('item features: %s\\nuser features: %s' % (num_item_features, num_user_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import  StringType\n",
    "\n",
    "vec_to_string_udf = udf(lambda value: ', '. join([str(d) for d in value]), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array, sum, count\n",
    "\n",
    "agg_test_ddf = test_ddf.groupBy('user', 'item').agg(array(*[sum(col('single_item_features')[i]) for i in range(num_item_features)]).alias('item_features_vector'), array(*[sum(col('single_user_features')[i]) for i in range(num_user_features)]).alias('user_features_vector'), count(col('item')).alias('num_bookings')).select('user', 'item', 'num_bookings', vec_to_string_udf('item_features_vector').alias('item_features'), vec_to_string_udf('user_features_vector').alias('user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train_ddf = train_ddf.groupBy('user', 'item').agg(array(*[sum(col('single_item_features')[i]) for i in range(num_item_features)]).alias('item_features_vector'), array(*[sum(col('single_user_features')[i]) for i in range(num_user_features)]).alias('user_features_vector'), count(col('item')).alias('num_bookings')).select('user', 'item', 'num_bookings', vec_to_string_udf('item_features_vector').alias('item_features'), vec_to_string_udf('user_features_vector').alias('user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full_ddf = feature_ddf.groupBy('user', 'item').agg(array(*[sum(col('single_item_features')[i]) for i in range(num_item_features)]).alias('item_features_vector'), array(*[sum(col('single_user_features')[i]) for i in range(num_user_features)]).alias('user_features_vector'), count(col('item')).alias('num_bookings')).select('user', 'item', 'num_bookings', vec_to_string_udf('item_features_vector').alias('item_features'), vec_to_string_udf('user_features_vector').alias('user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_ddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ddf, rest_ddf = agg_train_ddf.randomSplit([0.05, 0.95], 411)\n",
    "dev_ddf.coalesce(1).write.mode('overwrite').csv('%s/dev_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_ddf.coalesce(1).write.mode('overwrite').csv('%s/evaluation_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train_ddf.coalesce(1).write.mode('overwrite').csv('%s/train_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full_ddf.coalesce(1).write.mode('overwrite').csv('%s/full_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical bookings in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, collect_list\n",
    "\n",
    "history_ddf = train_ddf.groupby('user').agg(concat_ws(',', collect_list(col('item'))).alias('historical_destinations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ddf.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ddf.coalesce(1).write.mode('overwrite').csv('%s/train_user_history_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": "3"
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  },
  "name": "sia_features",
  "notebookId": 695
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
