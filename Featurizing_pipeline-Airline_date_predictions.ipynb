{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizing KLM data for date predictions\n",
    "This notebook will be the featurizing pipeline for the model which will be used to make predictions in regards to whether a user will convert in a given time-window.\n",
    "Specifically, the model is dealing only with conversions events, which means that all other events are not taken into account when making predictions with this model.\n",
    "\n",
    "For the model to work properly, the relevant fields which the model will look at are the origin, destination, travelClass, LOS, DBD, pax and the value of the deal. Based on those feautres, the model will be able to classify the users in the dataset with either 1 or 0 for a given time-window.\n",
    "\n",
    "#### Keep in mind:\n",
    "The time-window can be changed by rewriting the `target_formatter` (SQLTransformer) in order to give predictions over other time-windows.\n",
    "For the sake of evaluation, the model will make predictions over the time-window of `current_date - 160 days` in order to fit the current data present, so the model will try to predict which users will book on this time-window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '48G', 'executorMemory': '8G', 'executorCores': 2, 'numExecutors': 50, 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"48G\", \"executorMemory\": \"8G\", \"executorCores\": 2, \"numExecutors\": 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '48G', 'executorMemory': '8G', 'executorCores': 2, 'numExecutors': 50, 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>None</td><td>pyspark3</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"date_predictions_model\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"10g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: string, value: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.caseSensitive=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlC = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s3 location where featurized pipeline will be written to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 's3://r42-data-scientists/tbuaron/klm'\n",
    "model_dir = 'booking_dates'\n",
    "pipeline_name = 'featurizing_pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s3 location to where the parquet data is currently in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = 's3://r42-analysis-adhoc/klm-export-parquet/201803*'\n",
    "ddf = spark.read.parquet(parquet_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s3 location to where the columns mapping dictionary is in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_loc = ('%s/%s/date_predictions_dict.json' % (directory, model_dir))\n",
    "column_map = spark.read.json(dict_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list of transformers to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### renaming columns transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_transformer = SQLTransformer(statement='SELECT timestamp, CAST(trackId as string) AS user_id, CAST(interactionType as string) \\\n",
    "                                                AS interaction_type, CAST(value as int) AS item_value, \\\n",
    "                                                CAST(conversion_variables_destination as string) AS item_destination_conv, \\\n",
    "                                                CAST(conversion_variables_country as string) AS item_id_conv, \\\n",
    "                                                CAST(conversion_variables_origin as string) AS item_origin_conv, \\\n",
    "                                                CAST(conversion_variables_travelClass as string) AS item_travel_class_conv, \\\n",
    "                                                CAST(conversion_variables_tripCode as string) AS user_tripCode, \\\n",
    "                                                CAST(conversion_variables_booking_timestamp as date) AS item_booking_timestamp_conv, \\\n",
    "                                                CAST(conversion_variables_dd as date) AS item_departure_date_1_conv, \\\n",
    "                                                CAST(conversion_variables_rd as date) AS item_return_date_1_conv, \\\n",
    "                                                CAST(conversion_variables_returnDate as date) as item_return_date_2_conv, \\\n",
    "                                                CAST(conversion_variables_pax as int) as item_pax_conv FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions_transformer = SQLTransformer(statement=\"SELECT * FROM __THIS__ WHERE interaction_type == 'conversion'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_formatter = SQLTransformer(statement=\"SELECT *, nullif(item_origin_conv, '') as conversion_origin, nullif(item_destination_conv, '') as conversion_destination, nullif(item_pax_conv, '') as pax_cleaned FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pax transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_transformer = SQLTransformer(statement='SELECT *, CASE WHEN pax_cleaned IS NULL THEN 0 ELSE pax_cleaned END AS pax_ FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cast columns transformer\n",
    "keep in mind: this transformer cast `two` return dates, for other clients it is possible that we will need to remove one of them because other clients might have only one return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "casting_transformer = SQLTransformer(statement=\"SELECT *, CAST(item_booking_timestamp_conv AS date) AS booking_timestamp, CAST(item_departure_date_1_conv AS date) AS departure_date, CAST(item_return_date_1_conv AS date) AS returnDate1, CAST(item_return_date_2_conv AS date) AS returnDate2, CAST(pax_ AS int) AS pax FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### return date transformer\n",
    "###### KLM specific transformer\n",
    "In KLM data, there are two columns related to the `return_date` of a user, in some cases `return_date_1` will be present, in other cases `return_date_2` will be present. Therefore, this transformer is aimed on joining those two columns together whenever the `return_date` column is present and not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_date_formatter = SQLTransformer(statement='SELECT *, CASE WHEN returnDate1 is not NULL THEN SUBSTRING(returnDate1, 1, 10) WHEN returnDate2 is not NULL THEN SUBSTRING(returnDate2, 1, 10) END AS return_date FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### origin transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_formatter = SQLTransformer(statement=\"SELECT *, CASE WHEN conversion_origin is not NULL THEN SUBSTRING(conversion_origin, 1, 3) ELSE 'UND' END AS origin FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### destination transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_formatter = SQLTransformer(statement=\"SELECT *, CASE WHEN conversion_destination is not NULL THEN SUBSTRING(conversion_destination, 1, 3) ELSE 'UND' END AS destination FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class transformer\n",
    "##### KLM specific transformer. \n",
    "This transformer checks if a user is booked a business class ticket or a economy class ticket.  This transformer will need to be changed to another transformer which fits the way other clients decide on their travel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_formatter = SQLTransformer(statement=\"SELECT *, CASE WHEN SUBSTRING(item_travel_class_conv, 1, 1) IN ('J', 'C', 'D', 'I', 'Z', 'O') THEN 'business' ELSE 'economy' END AS class FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOS transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "los_formatter = SQLTransformer(statement=\"SELECT *, CASE WHEN return_date IS NULL THEN 1 ELSE DATEDIFF(return_date, departure_date) END AS LOS FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBD transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbd_formatter = SQLTransformer(statement=\"SELECT *, CASE WHEN departure_date IS NULL THEN 1 ELSE DATEDIFF(departure_date, booking_timestamp) END AS DBD FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value transformer\n",
    "price value of the tickets bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_transformer = SQLTransformer(statement='SELECT *, CASE WHEN item_value is not NULL THEN item_value ELSE 0 END as price_value FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### booking_type transformer\n",
    "concatinating `destination` column together with `class` column in order to generate unique `booking_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_typer = SQLTransformer(statement=\"SELECT *, CONCAT(destination, '_', class) AS booking_type FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target transformer \n",
    "keeping `-160` for now in order to be able to evaluate the model.\n",
    "For production, this will need to be changed in orde to fit the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_formatter = SQLTransformer(statement='SELECT *, CASE WHEN DATEDIFF(DATE_ADD(current_timestamp(), -160), booking_timestamp) BETWEEN -21 AND 0 THEN 1 ELSE 0 END AS label FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selected columns transformer\n",
    "keep only relevant columns in the dataframe before fitting it to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_selector = SQLTransformer(statement='SELECT user_id, timestamp, price_value, booking_timestamp, departure_date, return_date, LOS, DBD, pax, booking_type, origin, label FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concatenate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transformers = [columns_transformer] + [conversions_transformer] + [null_formatter] + [pax_transformer] + [casting_transformer] + [return_date_formatter] + [origin_formatter] + [destination_formatter] + [class_formatter] + [los_formatter] + [dbd_formatter] + [value_transformer] + [booking_typer] + [target_formatter] + [columns_selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['origin', 'booking_type']\n",
    "indexed_features = [feature + '_idx' for feature in categorical_features]\n",
    "encoded_features = [feature + '_enc' for feature in indexed_features]\n",
    "numerical_features = ['pax', 'LOS', 'DBD', 'price_value']\n",
    "booking_date_features = indexed_features + numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, VectorIndexer\n",
    "\n",
    "booking_date_indexers = [StringIndexer(inputCol=c, outputCol=c + '_idx', handleInvalid='keep') for c in categorical_features]\n",
    "booking_date_encoder = OneHotEncoderEstimator(inputCols=indexed_features, outputCols=encoded_features)\n",
    "booking_date_assembler = VectorAssembler(inputCols=booking_date_features, outputCol='features')\n",
    "\n",
    "featurizing_pipeline = Pipeline(stages = all_transformers + booking_date_indexers + [booking_date_encoder] \\\n",
    "                                         + [booking_date_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_featurizing = featurizing_pipeline.fit(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_ddf = fitted_featurizing.transform(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing featurized data to s3 takes approximately 1.2 hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_ddf.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save('%s/%s/%s' % (directory, model_dir, pipeline_name), header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
