{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"48G\", \"executorMemory\": \"48G\", \"executorCores\": 6, \"numExecutors\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.caseSensitive=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = ''\n",
    "feature_set_name = ''\n",
    "features_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "ddf = spark.read.parquet('%s/*' % source_dir)\n",
    "conversions_ddf = ddf.filter((ddf['interactionType'] == 'conversion'))\\\n",
    "         .dropDuplicates()\\\n",
    "         .replace('', 'undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlC = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    'trackId': 'user_id',\n",
    "    'conversion_variables_destination': 'item_destination_raw',\n",
    "    'conversion_variables_origin': 'item_origin_raw',\n",
    "    'conversion_variables_booking_timestamp': 'item_booking_timestamp',\n",
    "    'conversion_variables_dd': 'item_departure_date_1',\n",
    "    'conversion_variables_rd': 'item_return_date_1',\n",
    "    'conversion_variables_returnDate': 'item_return_date_2',\n",
    "    'conversion_variables_travelClass': 'item_travel_class_raw',\n",
    "    'conversion_variables_pax': 'item_pax_raw',\n",
    "    'conversion_variables_pos': 'user_location',\n",
    "    'conversion_variables_CustomerID': 'user_customer_id',\n",
    "    'conversion_variables_SocialAge': 'user_age_bracket',\n",
    "    'conversion_variables_AwardMiles': 'user_loyalty_points',\n",
    "    'conversion_variables_curr': 'user_currency',\n",
    "    'conversion_variables_country': 'user_country',\n",
    "    'conversion_variables_device': 'user_device',\n",
    "    'conversion_variables_language': 'user_language'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "base_ddf = conversions_ddf.select([col(k).alias(v) for k, v in column_map.items()]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversions_ddf.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversions_ddf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "\n",
    "\n",
    "def simple_map_operation(sql_context, f, columns_names_list, output_col_name, output_spark_type):\n",
    "    f = udf(f, output_spark_type)\n",
    "    return udf_to_transformer(sql_context, f, columns_names_list, output_col_name)\n",
    "\n",
    "\n",
    "def udf_to_transformer(sql_context, f, columns_names_list, output_col_name):\n",
    "    function_registered_name = f.__name__ + '_udf_version'\n",
    "    sql_context.udf.register(function_registered_name, f)\n",
    "\n",
    "    function_call = function_registered_name + str(tuple(columns_names_list)).replace(\"'\", '')\n",
    "    if len(columns_names_list) == 1:\n",
    "        function_call = function_call.replace(',', '')\n",
    "\n",
    "    sql_request = 'SELECT *, ' + function_call + ' AS ' + output_col_name + ' FROM __THIS__'\n",
    "    return SQLTransformer(statement=sql_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def airport_cleaner(airport):\n",
    "    if airport is not None and len(airport) >= 3:\n",
    "        return re.sub(r'\\W+', '', airport)\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_cleaner = simple_map_operation(sqlC, airport_cleaner, ['item_origin_raw'], 'item_origin', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_cleaner = simple_map_operation(sqlC, airport_cleaner, ['item_destination_raw'], 'item_destination', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_or_economy(travelClass):\n",
    "    business_class = ['J', 'C', 'D', 'I', 'Z', 'O']\n",
    "    try:\n",
    "        if any(x in business_class for x in travelClass):\n",
    "            return 'business'\n",
    "        else:\n",
    "            return 'economy'\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_transformer = simple_map_operation(sqlC, business_or_economy, ['item_travel_class_raw'], 'item_travel_class', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_assembler(origin, destination):\n",
    "    if origin is not None and destination is not None:\n",
    "        return origin + '-' + destination\n",
    "    else:\n",
    "        return 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_transformer = simple_map_operation(sqlC, item_assembler, ['item_origin', 'item_destination'], 'item_id', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pax_cleaner(item_pax_raw):\n",
    "    if item_pax_raw == 'undefined' or item_pax_raw is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(item_pax_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax_transformer = simple_map_operation(sqlC, pax_cleaner, ['item_pax_raw'], 'item_pax', IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def merge_date_columns(primary_column, secondary_column):\n",
    "    if primary_column and primary_column != 'undefined':\n",
    "        return dt.strptime(primary_column, '%Y-%m-%d')\n",
    "    elif secondary_column and secondary_column != 'undefined':\n",
    "        return dt.strptime(secondary_column, '%Y-%m-%d')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def merge_timestamp_columns(primary_column):\n",
    "    if primary_column and primary_column != 'undefined':\n",
    "        return dt.strptime(primary_column, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_columns(primary_column):\n",
    "    if primary_column and primary_column != 'undefined':\n",
    "        return dt.strptime(primary_column, '%Y-%m-%d')\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airline_membership(primary_column):\n",
    "    if primary_column > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_date_transformer = simple_map_operation(sqlC, date_columns, ['item_departure_date_1'], 'item_departure_date', DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_date_transformer = simple_map_operation(sqlC, merge_date_columns, ['item_return_date_1', 'item_return_date_2'], 'item_return_date', DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_timestamp_transformer = simple_map_operation(sqlC, merge_timestamp_columns, ['item_booking_timestamp'], 'item_booking_date', DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_transformer = simple_map_operation(sqlC, airline_membership, ['user_loyalty_points'], 'user_member', IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_transformers = [origin_cleaner] + [destination_cleaner] + [class_transformer] + [item_transformer] + [pax_transformer] + [departure_date_transformer] + [return_date_transformer] + [booking_timestamp_transformer] + [membership_transformer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic destination-recommendation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def days_between(start_date, end_date, no_end_date=1):\n",
    "    if start_date is not None and end_date is not None:\n",
    "        return (end_date - start_date).days\n",
    "    else:\n",
    "        return no_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "los_transformer = simple_map_operation(sqlC, days_between, ['item_departure_date', 'item_return_date'], 'item_los', IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbd_transformer = simple_map_operation(sqlC, days_between, ['item_departure_date', 'item_booking_date'], 'user_dbd', IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "def holiday_date_mapper(departure_date):\n",
    "    def calc_easter(year):\n",
    "        \"Returns Easter as a date object.\"\n",
    "        a = year % 19\n",
    "        b = year // 100\n",
    "        c = year % 100\n",
    "        d = (19 * a + b - b // 4 - ((b - (b + 8) // 25 + 1) // 3) + 15) % 30\n",
    "        e = (32 + 2 * (b % 4) + 2 * (c // 4) - d - (c % 4)) % 7\n",
    "        f = d + e - 7 * ((a + 11 * d + 22 * e) // 451) + 114\n",
    "        month = f // 31\n",
    "        day = f % 31 + 1\n",
    "        date_ = dt.date(year, month, day)\n",
    "        return date_\n",
    "    \n",
    "    if departure_date is not None and departure_date != 'undefined':\n",
    "        if departure_date.isocalendar()[1] in (51, 52):\n",
    "            return 'christmas'\n",
    "        elif departure_date.isocalendar()[1] == (1):\n",
    "            return 'new years'\n",
    "        elif departure_date.month == (7 or 8):\n",
    "            return 'summer'\n",
    "        elif departure_date == calc_easter(departure_date.year):\n",
    "            return 'easter'\n",
    "        else:\n",
    "            return 'no holiday'\n",
    "    else:\n",
    "        return 'no holiday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_transformer = simple_map_operation(sqlC, holiday_date_mapper, ['item_departure_date'], 'item_holiday', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drec_transformers = [los_transformer] + [dbd_transformer] + [holiday_transformer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['item_pax', 'item_los', 'user_dbd']\n",
    "categorical_features = ['item_travel_class', 'item_holiday', 'item_origin', 'item_destination',\\\n",
    "                        'user_member', 'user_age_bracket', 'user_country', 'user_language', 'user_currency']\n",
    "indexed_cat_cols = [c + '_idx' for c in categorical_features]\n",
    "ohe_cat_cols = [c.replace('_idx', '_') for c in indexed_cat_cols]\n",
    "user_features = list(filter(lambda c: c.startswith('user_'), ohe_cat_cols + numerical_features))\n",
    "item_features = list(filter(lambda c: c.startswith('item_'), ohe_cat_cols + numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, SQLTransformer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "user_indexer = StringIndexer(inputCol='user_id', outputCol='user_idx', handleInvalid='keep')\n",
    "item_indexer = StringIndexer(inputCol='item_id', outputCol='item_idx', handleInvalid='keep')\n",
    "feature_indexers = [StringIndexer(inputCol=col, outputCol=col +'_idx', handleInvalid='keep') for col in categorical_features]\n",
    "onehotencoders = OneHotEncoderEstimator(inputCols=indexed_cat_cols, outputCols=ohe_cat_cols, handleInvalid='keep', dropLast=False)\n",
    "item_feature_assembler = VectorAssembler(inputCols=item_features, outputCol='item_features')\n",
    "user_feature_assembler = VectorAssembler(inputCols=user_features, outputCol='user_features')\n",
    "\n",
    "features_pipeline = Pipeline(stages=airline_transformers + drec_transformers + [user_indexer] + [item_indexer] + feature_indexers + [onehotencoders] + [item_feature_assembler] + [user_feature_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = features_pipeline.fit(base_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "array_udf = udf(lambda value: value.toArray().tolist(), ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ddf = feature_model.transform(base_ddf).select(col('user_idx').cast('int').alias('user'), col('item_idx').cast('int').alias('item'), 'item_booking_date', 'item_departure_date', array_udf('item_features').alias('single_item_features'), array_udf('user_features').alias('single_user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model.stages[1].write().overwrite().save('%s/user_index_%s' % (features_dir, feature_set_name))\n",
    "feature_model.stages[2].write().overwrite().save('%s/item_index_%s' % (features_dir, feature_set_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unique users: %s; unique items: %s' % (feature_ddf.select('user').distinct().count(), feature_ddf.select('item').distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# departures_ddf = feature_ddf.groupBy('item_departure_date').count().withColumnRenamed('count', 'departures')\n",
    "# conversions_ddf = feature_ddf.groupBy('item_booking_date').count().withColumnRenamed('count', 'conversions')\n",
    "# display(departures_ddf.join(conversions_ddf, departures_ddf.item_departure_date == conversions_ddf.item_booking_date).withColumnRenamed('item_booking_date', 'date').select('date', 'conversions', 'departures').orderBy(col('date').asc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddf, test_ddf = feature_ddf.randomSplit([0.9, 0.1], seed = 93101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train observations: %s; evaluation observations: %s' % (train_ddf.count(), test_ddf.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "num_item_features = feature_ddf.select(size('single_item_features').alias('nif')).first().nif\n",
    "num_user_features = feature_ddf.select(size('single_user_features').alias('nuf')).first().nuf\n",
    "print('item features: %s\\nuser features: %s' % (num_item_features, num_user_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import  StringType\n",
    "\n",
    "vec_to_string_udf = udf(lambda value: ', '. join([str(d) for d in value]), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array, sum, count\n",
    "\n",
    "agg_test_ddf = test_ddf.groupBy('user', 'item').agg(array(*[sum(col('single_item_features')[i]) for i in range(num_item_features)]).alias('item_features_vector'), array(*[sum(col('single_user_features')[i]) for i in range(num_user_features)]).alias('user_features_vector'), count(col('item')).alias('num_bookings')).select('user', 'item', 'num_bookings', vec_to_string_udf('item_features_vector').alias('item_features'), vec_to_string_udf('user_features_vector').alias('user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train_ddf = train_ddf.groupBy('user', 'item').agg(array(*[sum(col('single_item_features')[i]) for i in range(num_item_features)]).alias('item_features_vector'), array(*[sum(col('single_user_features')[i]) for i in range(num_user_features)]).alias('user_features_vector'), count(col('item')).alias('num_bookings')).select('user', 'item', 'num_bookings', vec_to_string_udf('item_features_vector').alias('item_features'), vec_to_string_udf('user_features_vector').alias('user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full_ddf = feature_ddf.groupBy('user', 'item').agg(array(*[sum(col('single_item_features')[i]) for i in range(num_item_features)]).alias('item_features_vector'), array(*[sum(col('single_user_features')[i]) for i in range(num_user_features)]).alias('user_features_vector'), count(col('item')).alias('num_bookings')).select('user', 'item', 'num_bookings', vec_to_string_udf('item_features_vector').alias('item_features'), vec_to_string_udf('user_features_vector').alias('user_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_ddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ddf, rest_ddf = agg_train_ddf.randomSplit([0.05, 0.95], 411)\n",
    "dev_ddf.coalesce(1).write.mode('overwrite').csv('%s/dev_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test_ddf.coalesce(1).write.mode('overwrite').csv('%s/evaluation_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train_ddf.coalesce(1).write.mode('overwrite').csv('%s/train_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_full_ddf.coalesce(1).write.mode('overwrite').csv('%s/full_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical bookings in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, collect_list\n",
    "\n",
    "history_ddf = train_ddf.groupby('user').agg(concat_ws(',', collect_list(col('item'))).alias('historical_destinations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ddf.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ddf.coalesce(1).write.mode('overwrite').csv('%s/train_user_history_%s' % (features_dir, feature_set_name), sep='\\t', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  },
  "name": "sia_features",
  "notebookId": 695.0
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
