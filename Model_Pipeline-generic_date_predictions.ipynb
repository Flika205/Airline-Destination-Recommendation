{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Predictions Model - Random Forest\n",
    "This model is aimed to predict whether a user which have already converted at least once, will book a ticket within a given time-window. This is done by taking into account some features such as origin, destination, travelClass, LOS, DBD, pax and the value of the deal.\n",
    "\n",
    "The model will learn from random set of users which have already booked at least one ticket in their past, and the model will output whether or not a user will book a ticket in a given time-window.\n",
    "The idea of this model is for the client to know whether or not they should include specific users in different campagins or exclude them for specific periods and regions.\n",
    "\n",
    "This model learns about the correlation between specific origins and destinations on the globe to booking dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"48G\", \"executorMemory\": \"8G\", \"executorCores\": 2, \"numExecutors\": 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"date_predictions_model\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"10g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.caseSensitive=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlC = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`directory` is the s3 bucket.\n",
    "\n",
    "`model_dir` is the model location in the s3 bucket.\n",
    "\n",
    "`featurized_pipeline_name` is the location of where the featured data can be read from.\n",
    "\n",
    "`predictions_pipeline_name` is where the predictions will be written to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert directory = ''\n",
    "assert model_dir = ''\n",
    "assert featurized_pipeline_name = ''\n",
    "assert predictions_pipeline_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read featurized dataframe from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = ('%s/%s/%s/*' % (directory, model_dir, featurized_pipeline_name))\n",
    "ddf = spark.read.parquet(file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select relevant columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "selector_transformer = SQLTransformer(statement='SELECT user_id, features, label FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_dataframe = SQLTransformer(statement='SELECT * FROM __THIS__ ORDER BY RAND()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add column with random values for splitting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = SQLTransformer(statement='SELECT *, RAND() AS random_var FROM __THIS__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataframe to train/test sets (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = SQLTransformer(statement='SELECT *, CASE WHEN random_var < 0.8 THEN \"train\" ELSE \"test\" END AS train_test FROM __THIS__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "pipeline = Pipeline(stages=[selector_transformer] + [shuffle_dataframe] + [rand] + [split_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = pipeline.fit(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_ddf = fitted_model.transform(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select rows for the training set - 80% of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = SQLTransformer(statement='SELECT user_id, features, label FROM __THIS__ WHERE train_test == \"train\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = Pipeline(stages=[train_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = train_pipeline.fit(bookings_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_booking_date_ddf = train_model.transform(bookings_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select rows for the testing set - 20% of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformer = SQLTransformer(statement='SELECT user_id, features, label FROM __THIS__ WHERE train_test == \"test\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "test_pipeline = Pipeline(stages=[test_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_pipeline.fit(bookings_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_booking_date_ddf = test_model.transform(bookings_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier\n",
    "explaination about the differnt hyperparameters: \n",
    "\n",
    "`numTrees` parameter is set to 50 due to it is the minimal number of trees in the forest while the performance of the model stays high (higher number of trees can improve performance of the model, but will cost more computation time).\n",
    "\n",
    "`maxBins` parameter is related to the amount of values in the categorical features. It has to be at least as large as the number of values in each categorical feature (OneHotEncoded `origin`, OneHotEncoded `booking_type`).\n",
    "\n",
    "`maxDepth` parameter is the maximum depth of the each tree in the forest, the deeper the tree, the higher the accuracy of the model (risking of overfitting the model on training set examples).\n",
    "\n",
    "`minInfoGain` parameter is related to the minimum information gain for a split to be considered at a tree node. the higher the value, the more risk for underfitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",  probabilityCol='conversion_probability' numTrees=50, maxBins=1250, maxDepth=30, minInfoGain=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(train_booking_date_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the features importances of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "featureImportances: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_date_predictions_ddf = rf_model.transform(test_booking_date_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns for evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_transformer = SQLTransformer(statement='SELECT user_id, label, prediction, conversion_probability FROM __THIS__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pipeline = Pipeline(stages=[predictions_transformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model = predictions_pipeline.fit(booking_date_predictions_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ddf = predictions_model.transform(booking_date_predictions_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write predictions to S3 as parquet in order to Evaluate using scala notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ddf.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save('%s/%s/%s' % (directory, model_dir, model_pipeline_name), header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
